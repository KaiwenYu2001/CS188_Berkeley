class RegressionModel(object):
    """
    A neural network model for approximating a function that maps from 
real
    numbers to real numbers. The network should be sufficiently large to 
be able
    to approximate sin(x) on the interval [-2pi, 2pi] to reasonable 
precision.
    """
    def __init__(self):
        # Initialize your model parameters here
        "*** YOUR CODE HERE ***"
        #self.hidden_size =512
        self.learning_rate = 0.05
        self.batch_size = 100

        self.w1 = nn.Parameter(1, 200)
        self.b1 = nn.Parameter(1, 200)
        self.w2 = nn.Parameter(200, 1)
        self.b2 = nn.Parameter(1, 1)

    def run(self, x):
        """
        Runs the model for a batch of examples.

        Inputs:
            x: a node with shape (batch_size x 1)
        Returns:
            A node with shape (batch_size x 1) containing predicted 
y-values
        """
        "*** YOUR CODE HERE ***"
        h = nn.Linear(x, self.w1)
        h = nn.AddBias(h, self.b1)
        h = nn.ReLU(h)
        y = nn.Linear(h, self.w2)
        y = nn.AddBias(y, self.b2)
        return y

    def get_loss(self, x, y):
        """
        Computes the loss for a batch of examples.

        Inputs:
            x: a node with shape (batch_size x 1)
            y: a node with shape (batch_size x 1), containing the true 
y-values
                to be used for training
        Returns: a loss node
        """
        "*** YOUR CODE HERE ***"
        y_pred = self.run(x)
        loss = nn.SquareLoss(y_pred, y)
        return loss


    def train(self, dataset):
        """
        Trains the model.
        """
        "*** YOUR CODE HERE ***"
        for x, y in dataset.iterate_forever(self.batch_size):
            loss = self.get_loss(x, y)
            grad_wrt_w1, grad_wrt_b1, grad_wrt_w2, grad_wrt_b2 = 
nn.gradients(loss,[self.w1, self.b1, self.w2, self.b2])

            self.w1.update(grad_wrt_w1, -self.learning_rate)
            self.b1.update(grad_wrt_b1, -self.learning_rate)
            self.w2.update(grad_wrt_w2, -self.learning_rate)
            self.b2.update(grad_wrt_b2, -self.learning_rate)

            if nn.as_scalar(loss) < 0.02:
                break
